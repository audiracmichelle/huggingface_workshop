{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Opensource LLMs on Huggingface\n",
    "\n",
    "This notebook demonstrates how to explore and use opensource Large Language Models (LLMs) from Huggingface Hub.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Exploring Available Models](#exploring)\n",
    "3. [Loading Model Weights](#loading)\n",
    "4. [Using Models for Inference](#inference)\n",
    "5. [Working with Different Model Types](#model-types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation <a name=\"setup\"></a>\n",
    "\n",
    "First, let's import the necessary libraries and check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers torch huggingface-hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import HfApi, list_models, model_info\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring Available Models <a name=\"exploring\"></a>\n",
    "\n",
    "Huggingface Hub hosts thousands of opensource models. Let's explore how to search and filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Huggingface Hub API\n",
    "api = HfApi()\n",
    "\n",
    "# Search for text-generation models\n",
    "models = list(list_models(\n",
    "    task=\"text-generation\",\n",
    "    sort=\"downloads\",\n",
    "    limit=10\n",
    "))\n",
    "\n",
    "print(\"Top 10 Most Downloaded Text Generation Models:\")\n",
    "print(\"=\" * 80)\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"{i}. {model.id}\")\n",
    "    print(f\"   Downloads: {model.downloads if hasattr(model, 'downloads') else 'N/A'}\")\n",
    "    print(f\"   Likes: {model.likes if hasattr(model, 'likes') else 'N/A'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about a specific model\n",
    "model_id = \"gpt2\"\n",
    "info = model_info(model_id)\n",
    "\n",
    "print(f\"Model Information for '{model_id}':\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model ID: {info.id}\")\n",
    "print(f\"Task: {info.pipeline_tag}\")\n",
    "print(f\"Library: {info.library_name}\")\n",
    "print(f\"Downloads: {info.downloads}\")\n",
    "print(f\"Likes: {info.likes}\")\n",
    "print(f\"Tags: {info.tags[:5] if info.tags else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Model Weights <a name=\"loading\"></a>\n",
    "\n",
    "Let's learn how to load model weights from Huggingface Hub. We'll use GPT-2 as an example since it's lightweight and popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load model and tokenizer directly\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This may take a moment as weights are downloaded...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(f\"✓ Model loaded\")\n",
    "\n",
    "# Check model size\n",
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel has {num_parameters:,} parameters\")\n",
    "print(f\"Model size: ~{num_parameters * 4 / 1e9:.2f} GB (fp32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load model with specific configurations\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# Load configuration\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  Hidden size: {config.n_embd}\")\n",
    "print(f\"  Number of layers: {config.n_layer}\")\n",
    "print(f\"  Number of attention heads: {config.n_head}\")\n",
    "print(f\"  Max position embeddings: {config.n_positions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Load model with reduced precision to save memory\n",
    "print(\"Loading model with half precision (fp16)...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(f\"✓ Model loaded in fp16\")\n",
    "print(f\"Memory footprint reduced by ~50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Models for Inference <a name=\"inference\"></a>\n",
    "\n",
    "Now let's use the loaded model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text generation\n",
    "prompt = \"Artificial intelligence is\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"\\nGenerated text:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the pipeline API (easier interface)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model_name, tokenizer=model_name)\n",
    "\n",
    "# Generate multiple variations\n",
    "prompts = [\n",
    "    \"The future of technology is\",\n",
    "    \"In the world of machine learning,\",\n",
    "    \"Open source software enables\"\n",
    "]\n",
    "\n",
    "print(\"Generated texts using pipeline:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generator(prompt, max_length=40, num_return_sequences=1)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Different Model Types <a name=\"model-types\"></a>\n",
    "\n",
    "Let's explore different types of models available on Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: BERT for masked language modeling\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "\n",
    "# Use BERT to predict masked words\n",
    "text = \"The capital of France is [MASK].\"\n",
    "inputs = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Get the predicted token\n",
    "mask_token_index = (inputs.input_ids == bert_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_token = bert_tokenizer.decode(predicted_token_id)\n",
    "\n",
    "print(f\"BERT Model Example:\")\n",
    "print(f\"Input: {text}\")\n",
    "print(f\"Prediction: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Smaller LLM models for resource-constrained environments\n",
    "print(\"\\nExploring smaller LLM models:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "small_models = [\n",
    "    \"distilgpt2\",           # Distilled version of GPT-2 (smaller, faster)\n",
    "    \"gpt2-medium\",          # Medium-sized GPT-2\n",
    "    \"EleutherAI/gpt-neo-125M\"  # GPT-Neo 125M parameters\n",
    "]\n",
    "\n",
    "for model_id in small_models:\n",
    "    try:\n",
    "        info = model_info(model_id)\n",
    "        print(f\"\\n{model_id}:\")\n",
    "        print(f\"  Pipeline: {info.pipeline_tag}\")\n",
    "        print(f\"  Downloads: {info.downloads}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model_id}: Could not fetch info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using DistilGPT-2 (lighter version)\n",
    "distil_model_name = \"distilgpt2\"\n",
    "distil_tokenizer = AutoTokenizer.from_pretrained(distil_model_name)\n",
    "distil_model = AutoModelForCausalLM.from_pretrained(distil_model_name)\n",
    "\n",
    "# Compare model sizes\n",
    "gpt2_params = sum(p.numel() for p in model.parameters())\n",
    "distil_params = sum(p.numel() for p in distil_model.parameters())\n",
    "\n",
    "print(f\"\\nModel Size Comparison:\")\n",
    "print(f\"GPT-2: {gpt2_params:,} parameters\")\n",
    "print(f\"DistilGPT-2: {distil_params:,} parameters\")\n",
    "print(f\"Reduction: {(1 - distil_params/gpt2_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Model Usage\n",
    "\n",
    "### Model Selection Criteria:\n",
    "1. **Task Type**: Choose models based on your specific task (text generation, classification, QA, etc.)\n",
    "2. **Model Size**: Balance between performance and resource constraints\n",
    "3. **License**: Check model licenses for commercial use restrictions\n",
    "4. **Community Support**: Popular models have better documentation and community support\n",
    "\n",
    "### Performance Optimization:\n",
    "1. **Use quantization**: Load models in fp16 or int8 for memory savings\n",
    "2. **Batch processing**: Process multiple inputs together when possible\n",
    "3. **Caching**: Save downloaded models locally to avoid re-downloading\n",
    "4. **GPU acceleration**: Use CUDA when available for faster inference\n",
    "\n",
    "### Exploring More Models:\n",
    "- Visit [Huggingface Model Hub](https://huggingface.co/models)\n",
    "- Filter by task, library, language, and license\n",
    "- Check model cards for detailed information\n",
    "- Read the documentation and community discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to inspect model weights\n",
    "def inspect_model_weights(model, layer_name=None):\n",
    "    \"\"\"\n",
    "    Inspect the weights of a model or specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded model\n",
    "        layer_name: Optional specific layer name to inspect\n",
    "    \"\"\"\n",
    "    print(\"Model Architecture:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if layer_name is None or layer_name in name:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"  Shape: {param.shape}\")\n",
    "            print(f\"  Parameters: {param.numel():,}\")\n",
    "            print(f\"  Dtype: {param.dtype}\")\n",
    "            print(f\"  Requires grad: {param.requires_grad}\")\n",
    "            print()\n",
    "            total_params += param.numel()\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    return total_params\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nInspecting first few layers of GPT-2:\")\n",
    "inspect_model_weights(model, layer_name=\"transformer.wte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- How to explore and search for models on Huggingface Hub\n",
    "- Different methods to load model weights\n",
    "- How to use models for inference\n",
    "- Working with different model architectures\n",
    "- Best practices for model usage\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different models for your specific use case\n",
    "2. Fine-tune models on your own data\n",
    "3. Explore model quantization and optimization techniques\n",
    "4. Deploy models to production environments\n",
    "\n",
    "### Resources:\n",
    "- [Huggingface Documentation](https://huggingface.co/docs)\n",
    "- [Transformers Library](https://github.com/huggingface/transformers)\n",
    "- [Model Hub](https://huggingface.co/models)\n",
    "- [Datasets Hub](https://huggingface.co/datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
